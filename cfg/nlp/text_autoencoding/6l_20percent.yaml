general: 
    trainer: "text-autoencoding"
    seed: 123
    logdir_base: "{{project_root}}/tensorboard"
    note: "test"

model:
    ckpt_dir: "{{project_root}}/checkpoints"
    save_checkpoint: True
    keep_higher_eval: False
    #load_checkpoint: "/data/john/projects/mltoolkit/debug/ckpt/checkpoint.pt"
    pos_encoding_type: 'learned'
    embedding_dim: 768
    dim_feed_forward: 768
    dropout: .05
    num_encoder_layers: 6
    num_decoder_layers: 6
    devices: 
        - "cuda:0"
        - "cuda:2"
        #- "cuda:3"
    freeze_encoder: True
    deconv_sequence:
        #- 512
        - 256
    deconv_kernel_sizes:
        - 33
    decode_style: "conv"
    relu_after_expansion: True

optim:
    lr: 3.0e-4
    weight_decay: 3.0e-6
    beta1: .9
    beta2: .999
    eps: 1.0e-8
    clip_max_norm: 1.5

    # optimizer scheduler
    sched_step_size: 200
    sched_gamma: 0.99

    # swa scheduler
    swa_begin: -1
    swa_strat_is_linear: True
    swa_anneal_epochs: 5
    swa_lr: 0.05
    swa_bn_update_steps: "all"

data: 
    num_proc: 24
    pin_memory: False
    num_shards: 1
    cache_dir: "{{project_root}}/data/cache"
    ds_save_dir: "{{project_root}}/data/text_ae"
    shuffle: True
    train_batch_size: 48
    val_batch_size: 48
    num_epochs: 2
    eval_freq: 10000
    log_freq: 2000
    inference_freq: 300
    train_test_split: .999
    tokenizer_name: "sentence-transformers/all-mpnet-base-v2"
    max_seq_len: 256
    do_mask: True
    mask_prob: .2

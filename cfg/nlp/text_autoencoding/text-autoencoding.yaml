general: 
    trainer: "text-autoencoding"
    seed: 123
    logdir_base: "{{project_root}}/tensorboard"
    note: "test"

model:
    ckpt_dir: "{{project_root}}/checkpoints"
    save_checkpoint: True
    keep_higher_eval: False
    #load_checkpoint: "/data/john/projects/mltoolkit/debug/ckpt/checkpoint.pt"

    embedding_dim: 768
    dim_feed_forward: 768
    mlp_dropout: .05
    decoder_dropout: .05
    num_decoder_layers: 12
    mlp_layers: 4
    devices: 
        - "cuda:0"
        - "cuda:1"
        - "cuda:2"
        - "cuda:3"
    freeze_encoder: True
    encoder: "all-mpnet-base-v1"
    encoder_batch_size: 100
    embeddings_on_cpu: True
    up_conv_sequence:
        - 16
        - 64
        - 128
      #- 512
    up_conv_kernel_sizes:
        - 5
        - 9
        - 33

optim:
    lr: 8.0e-5
    weight_decay: 5.0e-5
    beta1: .9
    beta2: .999
    eps: 1.0e-8
    clip_max_norm: 1.5

    # optimizer scheduler
    sched_step_size: 200
    sched_gamma: 0.99

    # swa scheduler
    swa_begin: -1
    swa_strat_is_linear: True
    swa_anneal_epochs: 5
    swa_lr: 0.05
    swa_bn_update_steps: "all"

data: 
    num_proc: 16
    pin_memory: False
    num_shards: 1
    cache_dir: "{{project_root}}/data/wikipedia"
    shuffle: True
    batch_size: 4
    num_epochs: 5
    eval_freq: 500
    log_freq: 10
    train_test_split: .98
    tokenizer_name: "sentence-transformers/all-mpnet-base-v2"
    commute_prob: 0.5
    max_seq_len: 512
    doc_model: "all-mpnet-base-v1"
    limit_sentences: True
    training_sample_limit: 50

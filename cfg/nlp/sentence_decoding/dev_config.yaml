general: 
    trainer: "sentence-decoding"
    seed: 123
    logdir_base: "{{project_root}}/tensorboard"
    note: "test"

model:
    ckpt_dir: "{{project_root}}/checkpoints"
    save_checkpoint: True
    keep_higher_eval: False
    #load_checkpoint: "/data/john/projects/mltoolkit/debug/ckpt/checkpoint.pt"

    embedding_dim: 768
    dim_feed_forward: 768
    mlp_dropout: .05
    decoder_dropout: .05
    num_decoder_layers: 3
    mlp_layers: 4
    devices: 
        - "cuda:2"
    freeze_encoder: True
    encoder: "all-mpnet-base-v1"
    encoder_batch_size: 100
    embeddings_on_cpu: True
    up_conv_sequence:
        - 512
    up_conv_kernel_sizes:
        - 33

optim:
    lr: 3.0e-4
    weight_decay: 3.0e-6
    beta1: .9
    beta2: .999
    eps: 1.0e-8
    clip_max_norm: 1.5

    # optimizer scheduler
    sched_step_size: 200
    sched_gamma: 0.99

    # swa scheduler
    swa_begin: -1
    swa_strat_is_linear: True
    swa_anneal_epochs: 5
    swa_lr: 0.05
    swa_bn_update_steps: "all"

data: 
    num_proc: 16
    pin_memory: False
    num_shards: 1
    cache_dir: "{{project_root}}/data/wikipedia"
    shuffle: True
    train_batch_size: 32
    val_batch_size: 48
    num_epochs: 2
    eval_freq: 2000
    log_freq: 50
    inference_freq: 300
    train_test_split: .95
    tokenizer_name: "sentence-transformers/all-mpnet-base-v2"
    max_seq_len: 512
    doc_model: "all-mpnet-base-v1"
    overlap_data_path: "{{project_root}}/data/synthetic_overlap/synthetic_overlap_data.csv"

paths:
  logdir_base: "{{project_root}}/tensorboard"
  cache: "{{project_root}}/data/cache"
  results: "{{project_root}}/results"
  
general: 
  task: "transformer_ae"
  seed: 127
  note: "a quick note about this training run. this note will show up in tensorboard."

params:
  save_criterion: "min"
  save_checkpoint: True

  # model params
  mask_prob: .33464
  freeze_encoder: True
  seq_len: 256
  dropout: 0.1
  # pooling operation
  pool_op: "attention"
  #pool_op: "average"
  n_pool_heads: 12
  d_attention: 768
  # choose upsampling procedure
  upsampling_proc: "linear"
  # "conv" upsampling params
  deconv_seq: [16, 64, 128, 256]
  deconv_kernels: [5, 9, 33, 65]
  deconv_activation: "relu"
  # "linear" upsampling params
  uplin_seq_len: 256
  uplin_activation: "relu"
  # decoder params
  decoder_n_layers: 12
  decoder_d_ffn: 768
  decoder_activation: 'gelu'
  decoder_n_heads: 12
  # classifier params
  cls_d_ffn: 1024
  cls_activation: "relu"

  dtype: "float32"

  # optimizer params
  lr: 6.06221e-5
  weight_decay: .0061027885
  beta1: .9
  beta2: .999
  eps: 1.0e-8
  clip_max_norm: 1

  # optimizer scheduler
  sched_step_size: 5
  sched_gamma: 0.9

  # data params
  num_proc: 20
  shuffle: True
  batch_size: 32
  num_epochs: 2
  eval_freq: 300
  log_freq: 50
  num_test_samples: 25

  # misc options
  skip_first_eval: True

# Used for hyperparameter search.
# For each parameter, if "num_samples" isn't specified, 
# only the values specificed in the "values" argument will be used
search_params:
  search_type: 'random'
  search_steps: 60
  train_step_limit: 900
  mask_prob: 
    values: [0, 0.6]
    num_samples: 10
    dtype: "float"
  weight_decay:
    values: [1.0e-5, 1.0e-2]
    num_samples: 10
    dtype: "float"
  lr: 
    values: [1.0e-5, 1.0e-3]
    num_samples: 10
    dtype: "float"

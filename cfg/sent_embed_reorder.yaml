paths:
  logdir_base: "{{project_root}}/tensorboard"
  cache: "{{project_root}}/data/cache"
  results: "{{project_root}}/results"
  
general: 
  task: "sent_emb_reorder"
  seed: 129
  note: "a quick note about this training run. this note will show up in tensorboard."

params:
  save_criterion: "min"
  save_checkpoint: True

  # model params
  d_model: 1024
  nhead: 16
  dim_feedforward: 2048
  #dropout: 0.01
  dropout: 0.0010464151
  activation: 'gelu'
  n_layers: 1
  n_positions: 64
  max_length: 64
  enc_batchsize: 64

  dtype: "float32"

  # optimizer params
  #lr: .0001519354
  #weight_decay: 0.0001519353699381043
  lr: 0.000136149
  weight_decay: 1.0e-1
  beta1: .9
  beta2: .999
  eps: 1.0e-8
  clip_max_norm: 1

  # optimizer scheduler
  sched_step_size: 10
  sched_gamma: 0.99

  # data params
  num_proc: 20
  shuffle: True
  #batch_size: 128
  batch_size: 256
  num_epochs: 40
  eval_freq: 600
  log_freq: 100
  #num_test_samples: 25

  # misc options
  skip_first_eval: True

# Used for hyperparameter search.
# For each parameter, if "num_samples" isn't specified, 
# only the values specificed in the "values" argument will be used
search_params:
  search_type: 'random'
  search_steps: 60
  train_step_limit: 2400
  weight_decay:
    values: [1.0e-6, 1.0e-2]
    num_samples: 10
    dtype: "float"
  lr: 
    values: [1.0e-6, 1.0e-3]
    num_samples: 10
    dtype: "float"
  batch_size: 
    values: [32, 64, 128]
    num_samples: 10
    dtype: "int"

paths:
  logdir_base: "{{project_root}}/tensorboard"
  cache: "{{project_root}}/data/cache"
  results: "{{project_root}}/results"
  
general: 
  task: "sofsat/lora"
  seed: 126
  note: "a quick note about this training run. this note will show up in tensorboard."

params:
  save_criterion: "min"
  save_checkpoint: True

  # model params
  #base_model: "mistralai/Mistral-7B-v0.1"
  #base_model: "openai-community/gpt2"
  base_model: "openai-community/gpt2-xl"
  lora_r: 8
  lora_alpha: 8
  lora_dropout: 0.1
  seq_len: 512
  dtype: "bfloat16"

  # optimizer params
  lr: .0017765318
  weight_decay: 5.05198e-5
  clip_max_norm: 1

  # optimizer scheduler
  sched_step_size: 10
  sched_gamma: 0.995

  # data params
  num_proc: 4
  shuffle: True
  batch_size: 8
  num_epochs: 15
  eval_freq: 1000
  log_freq: 50

  # misc options
  skip_first_eval: True

# Used for hyperparameter search.
# For each parameter, if "num_samples" isn't specified, 
# only the values specificed in the "values" argument will be used
search_params:
  search_type: 'random'
  search_steps: 60
  train_step_limit: 900
  lr: 
    values: [8.1e-4, 1.0e-2]
    num_samples: 10
    dtype: "float"
  weight_decay:
    values: [1.0e-6, 1.0e-4]
    num_samples: 10
    dtype: "float"

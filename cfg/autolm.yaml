paths:
  logdir_base: "{{project_root}}/tensorboard"
  cache: "{{project_root}}/data/cache"
  results: "{{project_root}}/results"
  
general: 
  task: "autolm"
  seed: 125
  note: "a quick note about this training run. this note will show up in tensorboard."

params:
  save_criterion: "min"
  save_checkpoint: True

  # model params
  d_embed: 2048
  n_transformer_layers: 6
  n_head: 8
  dropout: .05
  dim_feed_forward:  2048
  tokenizer: "bert-base-uncased"
  seq_len: 256
  dtype: "bfloat16"

  # optimizer params
  lr: 5.0e-4
  weight_decay: 1.0e-4
  beta1: .9
  beta2: .999
  eps: 1.0e-8
  clip_max_norm: 1

  # optimizer scheduler
  sched_step_size: 1
  sched_gamma: 0.95

  # data params
  num_proc: 30
  shuffle: True
  batch_size: 42
  num_epochs: 4
  eval_freq: 200
  log_freq: 50
  train_test_split: .995

  # misc options
  skip_first_eval: False

# Used for hyperparameter search.
# For each parameter, if "num_samples" isn't specified, 
# only the values specificed in the "values" argument will be used
search_params:
  # search settings
  search_type: 'grid'
  step_limit: 1000

  # search params
  weight_decay:
    values: [1.0e-6, 1.0e-3]
    num_samples: 4
    dtype: "float"
  lr:
    values: [1.0e-4, 1.0e-3]
    dtype: "float"
